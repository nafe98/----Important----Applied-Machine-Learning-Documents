{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "1. Machine Learning and Types\n",
    "2. Application of Machine Learning\n",
    "3. Steps of Machine Learning\n",
    "4. Factors help to choose algorithm\n",
    "5. Algorithm\n",
    "         Linear Regression\n",
    "         TheilSenRegressor\n",
    "         RANSAC Regressor\n",
    "         HuberRegressor\n",
    "         Logistic Regression\n",
    "         GaussianProcessClassifier\n",
    "         Support Vector Machine\n",
    "         Nu-Support Vector Classification\n",
    "         Naive Bayes Algorithm\n",
    "         KNN\n",
    "         Perceptron\n",
    "         Random Forest\n",
    "         Decision Tree\n",
    "         Extra Tree\n",
    "         AdaBoost Classifier\n",
    "         PassiveAggressiveClassifier\n",
    "         Bagging Classifier\n",
    "         Gradient Boosting\n",
    "                 Light GBM\n",
    "                 XGBoost\n",
    "                 Catboost\n",
    "                 Stochastic Gradient Descent\n",
    "         Lasso\n",
    "         RidgeC lassifier CV\n",
    "         Kernel Ridge Regression\n",
    "         Bayesian Ridge\n",
    "         Elastic Net Regression\n",
    "         LDA\n",
    "         K-Means Algorithm\n",
    "         CNN\n",
    "         LSTM\n",
    "         PCA\n",
    "         Apriori\n",
    "         Prophet\n",
    "         ARIMA\n",
    "6. Evaluate Algorithms\n",
    "                \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.\n",
    "There are many algorithm for getting machines to learn, from using basic decision trees to clustering to layers of artificial neural networks depending on what task you’re trying to accomplish and the type and amount of data that you have available.  \n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are three types of machine learning** \n",
    "1. Supervised Machine Learning \n",
    "2. Unsupervised Machine Learning \n",
    "3. Reinforcement Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Supervised Machine Learning \n",
    "\n",
    "**It is a type of learning in which both input and desired output data are provided. Input and output data are labeled for classification to provide a learning basis for future data processing.This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data.   \n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning\n",
    "\n",
    "**Unsupervised learning is the training of an algorithm using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance.The main idea behind unsupervised learning is to expose the machines to large volumes of varied data and allow it to learn and infer from the data. However, the machines must first be programmed to learn from data. **\n",
    "\n",
    "** Unsupervised learning problems can be further grouped into clustering and association problems.  \n",
    "**\n",
    "1. Clustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behaviour. \n",
    "2. Association: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy X also tend to buy Y. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Machine Learning \n",
    "**Reinforcement Learning is a type of Machine Learning which allows machines to automatically determine the ideal behaviour within a specific context, in order to maximize its performance. Simple reward feedback is required for the agent to learn its behaviour; this is known as the reinforcement signal.It differs from standard supervised learning, in that correct input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance, which involves finding a balance between exploration of uncharted territory and exploitation of current knowledge  \n",
    "**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Supervised Machine Learning \n",
    "1. Bioinformatics \n",
    "2. Quantitative structure \n",
    "3. Database marketing \n",
    "4. Handwriting recognition \n",
    "5. Information retrieval \n",
    "6. Learning to rank \n",
    "7. Information extraction \n",
    "8. Object recognition in computer vision \n",
    "9. Optical character recognition \n",
    "10. Spam detection \n",
    "11. Pattern recognition \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Unsupervised Machine Learning \n",
    "1. Human Behaviour Analysis \n",
    "2. Social Network Analysis to define groups of friends. \n",
    "3. Market Segmentation of companies by location, industry, vertical. \n",
    "4. Organizing computing clusters based on similar event patterns and processes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Reinforcement Machine Learning \n",
    "1. Resources management in computer clusters \n",
    "2. Traffic Light Control \n",
    "3. Robotics \n",
    "4. Web System Configuration \n",
    "5. Personalized Recommendations \n",
    "6. Deep Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can apply machine learning model by following six steps:-\n",
    "1. Problem Definition \n",
    "2. Analyse Data \n",
    "3. Prepare Data \n",
    "4. Evaluate Algorithm \n",
    "5. Improve Results \n",
    "6. Present Results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors help to choose algorithm \n",
    "1. Type of algorithm \n",
    "2. Parametrization \n",
    "3. Memory size \n",
    "4. Overfitting tendency \n",
    "5. Time of learning \n",
    "6. Time of predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "**It is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \n",
    "Y = a + bX where **\n",
    "* Y – Dependent Variable \n",
    "* a – intercept \n",
    "* X – Independent variable \n",
    "* b – Slope \n",
    "\n",
    "**Example: University GPA' = (0.675)(High School GPA) + 1.097**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "train = pd.read_csv(\"../input/random-linear-regression/train.csv\") \n",
    "test = pd.read_csv(\"../input/random-linear-regression/test.csv\") \n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model with plots and accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train.iloc[:, :-1].values)\n",
    "y_train = np.array(train.iloc[:, 1].values)\n",
    "X_test = np.array(test.iloc[:, :-1].values)\n",
    "y_test = np.array(test.iloc[:, 1].values)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "plt.plot(X_train, model.predict(X_train), color='green')\n",
    "plt.show()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TheilSen Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  TheilSenRegressor\n",
    "model = TheilSenRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANSAC Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  RANSACRegressor\n",
    "model = RANSACRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huber Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  HuberRegressor\n",
    "model = HuberRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "**It’s a classification algorithm, that is used where the response variable is categorical. The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.**   \n",
    "* odds= p(x)/(1-p(x)) = probability of event occurrence / probability of not event occurrence \n",
    "\n",
    "**Example- When we have to predict if a student passes or fails in an exam when the number of hours spent studying is given as a feature, the response variable has two values, pass and fail. \n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries and data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../input/titanic/train.csv\")\n",
    "test  = pd.read_csv('../input/titanic/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "ports = pd.get_dummies(train.Embarked , prefix='Embarked')\n",
    "train = train.join(ports)\n",
    "train.drop(['Embarked'], axis=1, inplace=True)\n",
    "train.Sex = train.Sex.map({'male':0, 'female':1})\n",
    "y = train.Survived.copy()\n",
    "X = train.drop(['Survived'], axis=1) \n",
    "X.drop(['Cabin'], axis=1, inplace=True) \n",
    "X.drop(['Ticket'], axis=1, inplace=True) \n",
    "X.drop(['Name'], axis=1, inplace=True) \n",
    "X.drop(['PassengerId'], axis=1, inplace=True)\n",
    "X.Age.fillna(X.Age.median(), inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter = 500000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "model = GaussianProcessClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine \n",
    "**Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.It is primarily a classier method that performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. SVM supports both regression and classification tasks and can handle multiple continuous and categorical variables \n",
    "**\n",
    "\n",
    "**Example: One class is linearly separable from the others like if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two dimensional space where each point has two co-ordinates **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "data_svm = pd.read_csv(\"../input/svm-classification/UniversalBank.csv\")\n",
    "data_svm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_svm.iloc[:,1:13].values\n",
    "y = data_svm.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nu Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import NuSVC\n",
    "nu_svm = pd.read_csv(\"../input/svm-classification/UniversalBank.csv\")\n",
    "nu_svm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nu_svm.iloc[:,1:13].values\n",
    "y = nu_svm.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "classifier = NuSVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm \n",
    "**A naive Bayes classifier is not a single algorithm, but a family of machine learning algorithms which use probability theory to classify data with an assumption of independence between predictors It is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods    \n",
    "**\n",
    "\n",
    "**Example: Emails are given and we have to find the spam emails from that.A spam filter looks at email messages for certain key words and puts them in a spam folder if they match.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "data = pd.read_csv('../input/classification-suv-dataset/Social_Network_Ads.csv')\n",
    "data_nb = data\n",
    "data_nb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian NB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_nb.iloc[:, [2,3]].values\n",
    "y = data_nb.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BernoulliNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_nb.iloc[:, [2,3]].values\n",
    "y = data_nb.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=BernoulliNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN \n",
    "**KNN does not learn any model. and stores the entire training data set which it uses as its representation.The output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction \n",
    "**\n",
    "\n",
    "**Example: Should the bank give a loan to an individual? Would an individual default on his or her loan? Is that person closer in characteristics to people who defaulted or did not default on their loans? **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries and Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = pd.read_csv(\"../input/iris/Iris.csv\")\n",
    "knn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = knn.iloc[:, [1,2,3,4]].values\n",
    "y = knn.iloc[:, 5].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "train = pd.read_csv(\"../input/random-linear-regression/train.csv\") \n",
    "test = pd.read_csv(\"../input/random-linear-regression/test.csv\") \n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "X_train = np.array(train.iloc[:, :-1].values)\n",
    "y_train = np.array(train.iloc[:, 1].values)\n",
    "X_test = np.array(test.iloc[:, :-1].values)\n",
    "y_test = np.array(test.iloc[:, 1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(n_neighbors=2)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** It is single layer neural network and used for classification **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "p = pd.read_csv(\"../input/iris/Iris.csv\")\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = p.iloc[:, [1,2,3,4]].values\n",
    "y = p.iloc[:, 5].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=Perceptron()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "**Random forest is collection of tress(forest) and it builds multiple decision trees and merges them together to get a more accurate and stable prediction.It can be used for both classification and regression problems.**\n",
    "\n",
    "**Example: Suppose we have a bowl of 100 unique numbers from 0 to 99. We want to select a random sample of numbers from the bowl. If we put the number back in the bowl, it may be selected more than once. \n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")\n",
    "rf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rf.drop('class', axis=1)\n",
    "y = rf['class']\n",
    "X = pd.get_dummies(X)\n",
    "y = pd.get_dummies(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "**Decision tree algorithm is classification algorithm under supervised machine learning and it is simple to understand and use in data.The idea of Decision tree is to split the big data(root) into smaller(leaves)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = data\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dt.iloc[:, [2,3]].values\n",
    "y = dt.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and  Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et = data\n",
    "et.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = et.iloc[:, [2,3]].values\n",
    "y = et.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=ExtraTreesClassifier(criterion=\"entropy\",random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ac = data\n",
    "ac.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accutacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ac.iloc[:, [2,3]].values\n",
    "y = ac.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=AdaBoostClassifier(random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "pac = data\n",
    "pac.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pac.iloc[:, [2,3]].values\n",
    "y = pac.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=PassiveAggressiveClassifier(random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = data\n",
    "bc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bc.iloc[:, [2,3]].values\n",
    "y = bc.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "classifier=BaggingClassifier(random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "**Gradient boosting is an alogithm under supervised machine learning, boosting means converting weak into strong. In this new tree is boosted over the previous tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = data\n",
    "gb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gb.iloc[:, [2,3]].values\n",
    "y = gb.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "gbk = GradientBoostingClassifier()\n",
    "gbk.fit(X_train, y_train)\n",
    "pred = gbk.predict(X_test)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:**\n",
    "\n",
    "1. Faster training speed and higher efficiency.\n",
    "2. Lower memory usage.\n",
    "3. Better accuracy.\n",
    "4. Support of parallel and GPU learning.\n",
    "5. Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n",
    "test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
    "data = pd.concat([train, test], sort=False)\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "nans=pd.isnull(data).sum()\n",
    "\n",
    "data['MSZoning']  = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n",
    "data['Utilities'] = data['Utilities'].fillna(data['Utilities'].mode()[0])\n",
    "data['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\n",
    "data['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n",
    "\n",
    "data[\"BsmtFinSF1\"]  = data[\"BsmtFinSF1\"].fillna(0)\n",
    "data[\"BsmtFinSF2\"]  = data[\"BsmtFinSF2\"].fillna(0)\n",
    "data[\"BsmtUnfSF\"]   = data[\"BsmtUnfSF\"].fillna(0)\n",
    "data[\"TotalBsmtSF\"] = data[\"TotalBsmtSF\"].fillna(0)\n",
    "data[\"BsmtFullBath\"] = data[\"BsmtFullBath\"].fillna(0)\n",
    "data[\"BsmtHalfBath\"] = data[\"BsmtHalfBath\"].fillna(0)\n",
    "data[\"BsmtQual\"] = data[\"BsmtQual\"].fillna(\"None\")\n",
    "data[\"BsmtCond\"] = data[\"BsmtCond\"].fillna(\"None\")\n",
    "data[\"BsmtExposure\"] = data[\"BsmtExposure\"].fillna(\"None\")\n",
    "data[\"BsmtFinType1\"] = data[\"BsmtFinType1\"].fillna(\"None\")\n",
    "data[\"BsmtFinType2\"] = data[\"BsmtFinType2\"].fillna(\"None\")\n",
    "\n",
    "data['KitchenQual']  = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\n",
    "data[\"Functional\"]   = data[\"Functional\"].fillna(\"Typ\")\n",
    "data[\"FireplaceQu\"]  = data[\"FireplaceQu\"].fillna(\"None\")\n",
    "\n",
    "data[\"GarageType\"]   = data[\"GarageType\"].fillna(\"None\")\n",
    "data[\"GarageYrBlt\"]  = data[\"GarageYrBlt\"].fillna(0)\n",
    "data[\"GarageFinish\"] = data[\"GarageFinish\"].fillna(\"None\")\n",
    "data[\"GarageCars\"] = data[\"GarageCars\"].fillna(0)\n",
    "data[\"GarageArea\"] = data[\"GarageArea\"].fillna(0)\n",
    "data[\"GarageQual\"] = data[\"GarageQual\"].fillna(\"None\")\n",
    "data[\"GarageCond\"] = data[\"GarageCond\"].fillna(\"None\")\n",
    "\n",
    "data[\"PoolQC\"] = data[\"PoolQC\"].fillna(\"None\")\n",
    "data[\"Fence\"]  = data[\"Fence\"].fillna(\"None\")\n",
    "data[\"MiscFeature\"] = data[\"MiscFeature\"].fillna(\"None\")\n",
    "data['SaleType']    = data['SaleType'].fillna(data['SaleType'].mode()[0])\n",
    "data['LotFrontage'].interpolate(method='linear',inplace=True)\n",
    "data[\"Electrical\"]  = data.groupby(\"YearBuilt\")['Electrical'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "data[\"Alley\"] = data[\"Alley\"].fillna(\"None\")\n",
    "\n",
    "data[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\n",
    "data[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)\n",
    "nans=pd.isnull(data).sum()\n",
    "nans[nans>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list = []\n",
    "for col in data.columns:\n",
    "    if type(data[col][0]) == type('str'): \n",
    "        _list.append(col)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "for li in _list:\n",
    "    le.fit(list(set(data[li])))\n",
    "    data[li] = le.transform(data[li])\n",
    "\n",
    "train, test = data[:len(train)], data[len(train):]\n",
    "\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "\n",
    "test = test.drop(columns=['SalePrice', 'Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, random_state = 2020, shuffle = True)\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "model_lgb.fit(X, y)\n",
    "r2_score(model_lgb.predict(X), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks.It is a perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#Data is used the same as LGB\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_xgb.fit(X, y)\n",
    "r2_score(model_xgb.predict(X), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Catboost is a type of gradient boosting algorithms which can  automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.Make sure you handle missing data well before you proceed with the implementation.\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "#Data is used the same as LGB\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model = CatBoostRegressor(iterations=500,\n",
    "                             learning_rate=0.05,\n",
    "                             depth=10,\n",
    "                             random_seed = 42,\n",
    "                             bagging_temperature = 0.2,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = 50,\n",
    "                             od_wait=20)\n",
    "cb_model.fit(X, y)\n",
    "r2_score(cb_model.predict(X), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic means random , so in Stochastic Gradient Descent dataset sample is choosedn random instead of the whole dataset.hough, using the whole dataset is really useful for getting to the minima in a less noisy or less random manner, but the problem arises when our datasets get really huge and for that SGD come in action**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "#Data is used the same as LGB\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = SGDRegressor(max_iter = 100)\n",
    "SGD.fit(X, y)\n",
    "r2_score(SGD.predict(X), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. Though originally defined for least squares, lasso regularization is easily extended to a wide variety of statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators, in a straightforward fashion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#Data is used the same as LGB\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "lasso.fit(X, y)\n",
    "r2_score(lasso.predict(X), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Classifier CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "#Data is used the same as LGB\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and  Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc = RidgeClassifierCV()\n",
    "rcc.fit(X, y)\n",
    "r2_score(rcc.predict(X), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KRR combine Ridge regression and classification with the kernel trick.It is similar to Support vector Regression but relatively very fast.This is suitable for smaller dataset (less than 100 samples)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#Data is used the same as LGB\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and  Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "KRR.fit(X, y)\n",
    "r2_score(KRR.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayesianRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Bayesian regression, is a regression model defined in probabilistic terms, with explicit priors on the parameters. The choice of priors can have the regularizing effect.Bayesian approach is a general way of defining and estimating statistical models that can be applied to different models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model  import BayesianRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#Data is used the same as LGB\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BR = BayesianRidge()\n",
    "BR.fit(X, y)\n",
    "r2_score(BR.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net Regression \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic net is a hybrid of ridge regression and lasso regularization.It combines feature elimination from Lasso and feature coefficient reduction from the Ridge model to improve your model's predictions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#Data is used the same as LGB\n",
    "X = train.drop(columns=['SalePrice', 'Id']) \n",
    "y = train['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "ENet.fit(X, y)\n",
    "r2_score(ENet.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.Itis  used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = data\n",
    "lda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lda.iloc[:, [2,3]].values\n",
    "y = lda.iloc[:, 4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "Model=LinearDiscriminantAnalysis()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm \n",
    "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data and the goal of this algorithm is to find groups in the data \n",
    "\n",
    "**Steps to use this algorithm:-**\n",
    "* 1-Clusters the data into k groups where k is predefined. \n",
    "* 2-Select k points at random as cluster centers. \n",
    "* 3-Assign objects to their closest cluster center according to the Euclidean distance function. \n",
    "* 4-Calculate the centroid or mean of all objects in each cluster. \n",
    "\n",
    "**Examples: Behavioral segmentation like segment by purchase history or by activities on application, website, or platform Separate valid activity groups from bots  **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = pd.read_csv(\"../input/k-mean/km.csv\")\n",
    "km.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_clusters = range(1,8)\n",
    "kmeans = [KMeans(n_clusters=i) for i in K_clusters]\n",
    "Y_axis = km[['latitude']]\n",
    "X_axis = km[['longitude']]\n",
    "score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]\n",
    "plt.plot(K_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, init ='k-means++')\n",
    "kmeans.fit(km[km.columns[1:3]])\n",
    "km['cluster_label'] = kmeans.fit_predict(km[km.columns[1:3]])\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.predict(km[km.columns[1:3]])\n",
    "km.cluster_label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting Clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.plot.scatter(x = 'latitude', y = 'longitude', c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "import tensorflow as tf\n",
    "train_data = pd.read_csv(\"../input/digit-recognizer/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/digit-recognizer/test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing and Data Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_data.drop(\"label\", axis=1)).astype('float32')\n",
    "y = np.array(train_data['label']).astype('float32')\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.xlabel(y[i])\n",
    "plt.show()\n",
    "\n",
    "X = X / 255.0\n",
    "X = X.reshape(-1, 28, 28, 1)\n",
    "y = to_categorical(y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "X_test = np.array(test_data).astype('float32')\n",
    "X_test = X_test / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "plt.figure(figsize=(10,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "model.summary()\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model1.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increse to epochs to 30 for better accuracy\n",
    "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=85, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.show()\n",
    "\n",
    "print(model.evaluate(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(X_test)\n",
    "submit = pd.DataFrame(prediction,columns=[\"Label\"])\n",
    "submit[\"ImageId\"] = pd.Series(range(1,(len(prediction)+1)))\n",
    "submission = submit[[\"ImageId\",\"Label\"]]\n",
    "submission.to_csv(\"submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM  blocks are part of a recurrent neural network structure. Recurrent neural networks are made to utilize certain types of artificial memory processes that can help these artificial intelligence programs to more effectively imitate human thought.It is  capable of learning order dependence \n",
    "LSTM can be used for machine translation, speech recognition, and more.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "lstm = pd.read_csv(\"../input/nyse/prices.csv\")\n",
    "lstm = lstm[lstm['symbol']==\"NFLX\"]\n",
    "lstm['date'] = pd.to_datetime(lstm['date'])\n",
    "lstm.set_index('date',inplace=True)\n",
    "lstm = lstm.reset_index()\n",
    "lstm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lstm.filter(['close'])\n",
    "dataset = data.values \n",
    "training_data_len = math.ceil(len(dataset)*.75)  \n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "train_data = scaled_data[0:training_data_len, :]\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(60,len(train_data)):\n",
    "    x_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i,0])\n",
    "x_train,y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "model.add(LSTM(64,return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "model.add(LSTM(64, return_sequences= False))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "from tensorflow.keras.utils import plot_model \n",
    "plot_model(model, to_file='model1.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train,y_train, batch_size=85, epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= scaled_data[training_data_len-60:, :]\n",
    "x_test = []\n",
    "y_test = dataset[training_data_len:,:]\n",
    "for i in range(60,len(test_data)):\n",
    "    x_test.append(test_data[i-60:i,0])\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))\n",
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's an important method for dimension reduction.It extracts low dimensional set of features from a high dimensional data set with a motive to capture as much information as possible and to visualise high-dimensional data, it also reduces noise and finally makes other algorithms to work better because we are injecting fewer inputs.**\n",
    "* Example: When we have to bring out strong patterns in a data set or to make data easy to explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import datasets\n",
    "class PCA:\n",
    "  def __init__(self, n_components):\n",
    "    self.n_components = n_components\n",
    "    self.components = None\n",
    "    self.mean = None\n",
    "\n",
    "  def fit(self, X):\n",
    "    self.mean = np.mean(X, axis=0)\n",
    "    X = X - self.mean\n",
    "    cov = np.cov(X.T)\n",
    "\n",
    "    evalue, evector = np.linalg.eig(cov)\n",
    "\n",
    "    eigenvectors = evector.T\n",
    "    idxs = np.argsort(evalue)[::-1]\n",
    "    \n",
    "    evalue = evalue[idxs]\n",
    "    evector = evector[idxs]\n",
    "    self.components = evector[0:self.n_components]\n",
    "\n",
    "  def transform(self, X):\n",
    "    #project data\n",
    "    X = X - self.mean\n",
    "    return(np.dot(X, self.components.T))\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "pca = PCA(2)\n",
    "pca.fit(X)\n",
    "X_projected = pca.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "x1 = X_projected[:,0]\n",
    "x2 = X_projected[:,1]\n",
    "\n",
    "plt.scatter(x1,x2,c=y,edgecolor='none',alpha=0.8,cmap=plt.cm.get_cmap('viridis',3))\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is a categorisation algorithm attempts to operate on database records, particularly transactional records, or records including certain numbers of fields or items.It is mainly used for sorting large amounts of data. Sorting data often occurs because of association rules. **\n",
    "* Example: To analyse data for frequent if/then patterns and using the criteria support and confidence to identify the most important relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/supermarket/GroceryStoreDataSet.csv',names=['products'],header=None)\n",
    "data = list(df[\"products\"].apply(lambda x:x.split(',')))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_data,columns=te.columns_)\n",
    "df1 = apriori(df,min_support=0.01,use_colnames=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Prophet is an extremely easy tool for analysts to produce reliable forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prophet only takes data as a dataframe with a ds (datestamp) and y (value we want to forecast) column. So first, let’s convert the dataframe to the appropriate format.\n",
    "1. Create an instance of the Prophet class and then fit our dataframe to it.\n",
    "2. Create a dataframe with the dates for which we want a prediction to be made with make_future_dataframe(). Then specify the number of days to forecast using the periods parameter.\n",
    "3. Call predict to make a prediction and store it in the forecast dataframe. What’s neat here is that you can inspect the dataframe and see the predictions as well as the lower and upper boundaries of the uncertainty interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import plot_plotly, add_changepoints_to_plot\n",
    "\n",
    "pred = pd.read_csv(\"../input/coronavirus-2019ncov/covid-19-all.csv\")\n",
    "pred = pred.fillna(0)\n",
    "predgrp = pred.groupby(\"Date\")[[\"Confirmed\",\"Recovered\",\"Deaths\"]].sum().reset_index()\n",
    "pred_cnfrm = predgrp.loc[:,[\"Date\",\"Confirmed\"]]\n",
    "pr_data = pred_cnfrm\n",
    "pr_data.columns = ['ds','y']\n",
    "pr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Forecast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Prophet()\n",
    "m.fit(pr_data)\n",
    "future=m.make_future_dataframe(periods=15)\n",
    "forecast=m.predict(future)\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_plotly(m, forecast)\n",
    "py.iplot(fig) \n",
    "\n",
    "fig = m.plot(forecast,xlabel='Date',ylabel='Confirmed Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "ar = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\n",
    "ar.date=ar.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n",
    "ar=ar.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n",
    "ar.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n",
    "ar=ar.reset_index()\n",
    "ar=ar.loc[:,[\"index\",\"item_cnt_day\"]]\n",
    "ar.columns = ['confirmed_date','count']\n",
    "ar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ar['count'].values, order=(1, 2, 1))\n",
    "fit_model = model.fit(trend='c', full_output=True, disp=True)\n",
    "fit_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model.plot_predict()\n",
    "plt.title('Forecast vs Actual')\n",
    "pd.DataFrame(fit_model.resid).plot()\n",
    "forcast = fit_model.forecast(steps=6)\n",
    "pred_y = forcast[0].tolist()\n",
    "pred = pd.DataFrame(pred_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate Algorithms** \n",
    "**The evaluation of algorithm consist three following steps:- **\n",
    "1. Test Harness  \n",
    "2. Explore and select algorithms \n",
    "3. Interpret and report results \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
